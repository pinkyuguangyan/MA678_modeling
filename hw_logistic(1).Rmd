---
title: "Homework 03"
author: "Guangyan Yu"
date: "September 11, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: Logistic Regression
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
#install.packages("pacman",repos="https://cloud.r-project.org")
pacman::p_load("ggplot2","knitr","arm","foreign","car","Cairo","data.table")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Data analysis 

### 1992 presidential election

The folder `nes` contains the survey data of presidential preference and income for the 1992 election analyzed in Section 5.1, along with other variables including sex, ethnicity, education, party identification, and political ideology.

```{r, echo=FALSE}
nes5200<-read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/nes/nes5200_processed_voters_realideo.dta")
#saveRDS(nes5200,"nes5200.rds")
#nes5200<-readRDS("nes5200.rds")

nes5200_dt <- data.table(nes5200)
  yr <- 1992
nes5200_dt_s<-nes5200_dt[ year==yr & presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_s<-nes5200_dt_s[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_s$income <- droplevels(nes5200_dt_s$income)
```

1.  Fit a logistic regression predicting support for Bush given all these inputs. Consider how to include these as regression predictors and also consider possible interactions.

```{r}
#bush is republican
#summary(nes5200_dt_s)
#change income to numeric variable
nes5200_dt_s<-nes5200_dt_s[!is.na(nes5200_dt_s$income)]
nes5200_dt_s$income<-as.numeric(nes5200_dt_s$income)
#change gender to numeric variable
nes5200_dt_s <- nes5200_dt_s[!is.na(nes5200_dt_s$gender)]
nes5200_dt_s$gender <- as.numeric(nes5200_dt_s$gender)
#race:through summary() we know race has NA, so first omit NA and turn to numeric variable
nes5200_dt_s <- nes5200_dt_s[!is.na(nes5200_dt_s$race)]
nes5200_dt_s$race<-as.numeric(nes5200_dt_s$race)
#educ1
nes5200_dt_s <- nes5200_dt_s[!is.na(nes5200_dt_s$educ1)]
nes5200_dt_s$educ1<-as.numeric(nes5200_dt_s$educ1)
#partyid7
nes5200_dt_s <- nes5200_dt_s[!is.na(nes5200_dt_s$partyid7)]
nes5200_dt_s$partyid7 <- as.numeric(nes5200_dt_s$partyid7)
#ideo
nes5200_dt_s <- nes5200_dt_s[!is.na(nes5200_dt_s$ideo)]
nes5200_dt_s$ideo<-as.numeric(nes5200_dt_s$ideo)

newdata<-nes5200_dt_s[,c("gender","race","income","partyid7","vote_rep","educ1","ideo")]
model1<-glm(vote_rep ~ gender * race + income * educ1 + partyid7 + ideo,family=binomial(link="logit"),data = newdata)
summary(model1)
model2<-glm(vote_rep ~ gender + race * ideo + income * educ1 + partyid7 ,family=binomial(link="logit"),data = newdata)
summary(model2)
```

2. Evaluate and compare the different models you have fit. Consider coefficient estimates and standard errors, residual plots, and deviances.

```{r}
predicted1 <- fitted(model1) #fitted(model) = predict(model,type="response")
predicted2 <- fitted(model2) #fitted(model) = predict(model,type="response")
#residul plot
plot(x=predicted1,y=resid(model1,type="response"))
abline(h=0,lty=3)
#binned residul plot
binnedplot(predicted1,resid(model1,type="response"))
#residual plot
plot(x=predicted2,y=resid(model2,type="response"))
abline(h=0,lty=3)
#binned residul plot
binnedplot(predicted2,resid(model2,type="response"))

er1<-mean ((predicted1>0.5 & newdata$vote_rep==0) | (predicted1<.5 & newdata$vote_rep==1))#error rate
er2<-mean ((predicted2>0.5 & newdata$vote_rep==0) | (predicted2<.5 & newdata$vote_rep==1))

er1
er2

```
*The binned residual plots are similar, both binned residuals are most fall in the 95% area.*

*The residual deviance of model1 is 679.6, the residual deviance of model2 is 689.3, indicating that model1 fits the data better.*

*The AIC of model1 is 697.64, the AIC of model2 is 707.27, indicating taht model1 fits the data better.*

3. For your chosen model, discuss and compare the importance of each input variable in the prediction.


*In model1, race,partyid,ideo,gender\*race are significant. *

*The variable race has the biggest estimated coefficient and the biggest standard error.*

### Graphing logistic regressions: 

the well-switching data described in Section 5.4 of the Gelman and Hill are in the folder `arsenic`.  

```{r, echo=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", header=TRUE)
wells_dt <- data.table(wells)
```

1. Fit a logistic regression for the probability of switching using log (distance to nearest safe well) as a predictor.
```{r}
model<-glm(switch ~ log(dist), family = binomial(link = "logit"),data = wells_dt)
summary(model)

```

2. Make a graph similar to Figure 5.9 of the Gelman and Hill displaying Pr(switch) as a function of distance to nearest safe well, along with the data.
```{r}
jitter.binary<-function(a,jitt=0.05){
  ifelse(a==0, runif(length(a),0,jitt), runif(length(a),1-jitt,1))
}
switch.jitter<-jitter.binary(wells_dt$switch)
dist_log<-log(wells_dt$dist)
plot(dist_log,switch.jitter)
curve(invlogit(coef(model)[1]+coef(model)[2]*x),add=TRUE)
```

3. Make a residual plot and binned residual plot as in Figure 5.13.
```{r}
plot(fitted(model),resid(model,type="response"))
abline(h=0,lty=3)
binnedplot(fitted(model),resid(model,type="response"))
```

4. Compute the error rate of the fitted model and compare to the error rate of the null model.

```{r}
predicted <- fitted(model)
mean((predicted>0.5 & wells_dt$switch==0)|(predicted<0.5 & wells_dt$switch==1))

model_null<-glm(switch~1,data=wells_dt,family=binomial(link = "logit"))
predicted_null<-fitted(model_null)
mean((predicted_null>0.5 & wells_dt$switch==0)|(predicted_null<0.5 & wells_dt$switch==1))

```

5. Create indicator variables corresponding to `dist < 100`, `100 =< dist < 200`, and `dist > 200`. Fit a logistic regression for Pr(switch) using these indicators. With this new model, repeat the computations and graphs for part (1) of this exercise.

```{r}
dist<-wells_dt$dist
dist[dist<100]<-1
dist[dist>=100 & dist<200]<-2
dist[dist>=200]<-3
model <- glm(wells_dt$switch ~ dist, family = binomial(link = "logit"))
summary(model)
jitter.binarynew<-function(a,jitt=0.05){
  a[a==1]<-runif(sum(a==1),1-jitt,1)
  a[a==2]<-runif(sum(a==2),2-jitt,2)
  a[a==3]<-runif(sum(a==3),3-jitt,3)
  return(a)
}
dist.jitter<-jitter.binarynew(dist)
#regression plot
plot(dist.jitter,switch.jitter)
curve(invlogit(coef(model)[1]+coef(model)[2]*x),add=TRUE)
#residual plot
plot(fitted(model),resid(model,type="response"))
abline(h=0,lty=3)
binnedplot(fitted(model),resid(model,type="response"))
#error rate
predicted <- fitted(model)
mean((predicted>0.5 & wells_dt$switch==0)|(predicted<0.5 & wells_dt$switch==1))
```

### Model building and comparison: 
continue with the well-switching data described in the previous exercise.

1. Fit a logistic regression for the probability of switching using, as predictors, distance, `log(arsenic)`, and their interaction. Interpret the estimated coefficients and their standard errors.

```{r}
switch<-wells_dt$switch
dist<-wells_dt$dist
arsenic<-wells_dt$arsenic
arsenic_log<-log(wells_dt$arsenic)
model<-glm(switch ~ dist * arsenic_log)
summary(model)
```
$logit^{-1}(0.61)=0.65$ *is the estimated probability of switching,if dist=0 and arsenic=1*

*0.002/4=0.0005 . With arsenic=1, each 1 meter of distance corresponds to an approximate 0.05% negative difference in probability of switching.*

*0.21/4=0.0525. With distance=0, each additional unit of log(arsenic) corresponds to an approximate 5.25% positive difference in probability of switching.*

*The importance of distance decreases by 0.04% for households with 1 unit hight existing log(arsenic) levels.*

2. Make graphs as in Figure 5.12 to show the relation between probability of switching, distance, and arsenic level.

```{r}
#P(switching) and distance
plot(dist,switch.jitter,xlim=c(0,max(dist)))
curve(invlogit(cbind(1,x,0.5,0.5*x) %*% coef(model)),add=TRUE)
curve(invlogit(cbind(1,x,-0.3,-0.3*x) %*% coef(model)),add=TRUE)
#P(switching) and arsenic level
plot(arsenic_log,switch.jitter,xlim=c(0,max(arsenic_log)))
curve(invlogit(cbind(1,0,x,0) %*% coef(model)),add=TRUE)
curve(invlogit(cbind(1,50,x,50*x) %*% coef(model)),add=TRUE)

```

3. Following the procedure described in Section 5.7, compute the average predictive differences corresponding to:
i. A comparison of dist = 0 to dist = 100, with arsenic held constant. 
ii. A comparison of dist = 100 to dist = 200, with arsenic held constant.
iii. A comparison of arsenic = 0.5 to arsenic = 1.0, with dist held constant. 
iv. A comparison of arsenic = 1.0 to arsenic = 2.0, with dist held constant.
Discuss these results.

```{r}
#dist=0,dist=100
coef<-coef(model)
a1<-0
a2<-100
delta<-invlogit(coef[1]+coef[2]*a1+coef[3]*arsenic_log+coef[4]*a1*arsenic_log)-invlogit(coef[1]+coef[2]*a2+coef[3]*arsenic_log+coef[4]*a2*arsenic_log)
print(mean(delta))
#dist=100,dist=200
a1<-100
a2<-200
delta<-invlogit(coef[1]+coef[2]*a1+coef[3]*arsenic_log+coef[4]*a1*arsenic_log)-invlogit(coef[1]+coef[2]*a2+coef[3]*arsenic_log+coef[4]*a2*arsenic_log)
print(mean(delta))
#arsenic=0.5,arsenic=1
b1<-log(0.5)
b2<-log(1)
delta<-invlogit(coef[1]+coef[2]*dist+coef[3]*b1+coef[4]*b1*dist)-invlogit(coef[1]+coef[2]*dist+coef[3]*b2+coef[4]*b2*dist)
print(mean(delta))
#arsenic=1,arsenic=2
b1<-log(1)
b2<-log(2)
delta<-invlogit(coef[1]+coef[2]*dist+coef[3]*b1+coef[4]*b1*dist)-invlogit(coef[1]+coef[2]*dist+coef[3]*b2+coef[4]*b2*dist)
print(mean(delta))
```
*On average in the data, householders that are 100 meters from the nearest safe well are 4.9% less likely to switch, compared to householders that are right next to the nearest safe well, at the same arsenic level.*

*On average in the data, householders that are 200 meters from the nearest safe well are 5.2% less likely to switch, compared to householders that are 100 meters from the nearest safe well, at the same arsenic level.*

*On average in the data, householders that have 1 level arsenic are 3.2% more likely to switch, compared to householders that have 0.5 level arsenic, at the same distance.*

*On average in the data, householders that have 2 level arsenic are 3.2% more likely to switch, compared to householders that have 1 level arsenic, at the same distance.*

### Building a logistic regression model: 
the folder rodents contains data on rodents in a sample of New York City apartments.

Please read for the data details.
http://www.stat.columbia.edu/~gelman/arm/examples/rodents/rodents.doc

```{r read_rodent_data, echo=FALSE}
apt.subset.data <- read.table ("http://www.stat.columbia.edu/~gelman/arm/examples/rodents/apt.subset.dat", header=TRUE)
apt_dt <- data.table(apt.subset.data)
setnames(apt_dt, colnames(apt_dt),c("y","defects","poor","race","floor","dist","bldg")
)
invisible(apt_dt[,asian := race==5 | race==6 | race==7])
invisible(apt_dt[,black := race==2])
invisible(apt_dt[,hisp  := race==3 | race==4])
```

1. Build a logistic regression model to predict the presence of rodents (the variable y in the dataset) given indicators for the ethnic groups (race). Combine categories as appropriate. Discuss the estimated coefficients in the model.

```{r}
asian<-as.numeric(apt_dt$asian)
black<-as.numeric(apt_dt$black)
hisp<-as.numeric(apt_dt$hisp)
y<-apt_dt$y
model1<-glm(y ~ asian + black + hisp, family = binomial(link = "logit"))
summary(model1)
```
*When the householder is not an asian or black or hisp, *$logit^{-1}(-2.15)$ *is the estimated probability of rodents infestation.*

*The odds ratio between odds of an asian housholder suffer from rodents and a housholder who is not asian or black or hisp, is exp(0.55)*

*The odds ratio between odds of an black housholder suffer from rodents and a housholder who is not asian or black or hisp, is exp(1.54)*

*The odds ratio between odds of an hisp housholder suffer from rodents and a housholder who is not asian or black or hisp, is exp(1.70)*

2. Add to your model some other potentially relevant predictors describing the apartment, building, and community district. Build your model using the general principles explained in Section 4.6 of the Gelman and Hill. Discuss the coefficients for the ethnicity indicators in your model.

```{r}
model<-glm(y~defects + poor + floor + asian + black + hisp, family = binomial(link = "logit"),data = apt_dt)
summary(model)
```
*With the consideration of predictors describing the apartment, the odds ratios described in the last question decrease. And there are difference between different races in the probability of suffering from rodents. Compared to people who are not asian or black or hisp, the hisp are more likely to have rodent infestation.*

# Conceptual exercises.

### Shape of the inverse logit curve

Without using a computer, sketch the following logistic regression lines:

1. $Pr(y = 1) = logit^{-1}(x)$
2. $Pr(y = 1) = logit^{-1}(2 + x)$
3. $Pr(y = 1) = logit^{-1}(2x)$
4. $Pr(y = 1) = logit^{-1}(2 + 2x)$
5. $Pr(y = 1) = logit^{-1}(-2x)$


### 
In a class of 50 students, a logistic regression is performed of course grade (pass or fail) on midterm exam score (continuous values with mean 60 and standard deviation 15). The fitted model is $Pr(pass) = logit^{-1}(-24+0.4x)$.

1. Graph the fitted model. Also on this graph put a scatterplot of hypothetical data consistent with the information given.

```{r}
x<-rnorm(50,60,15)
f<-function(x){
  result=invlogit(-24+0.4*x)
  return(result)
}
y<-ifelse(f(x)>=0.5,1,0)
jitter.binary<-function(a,jitt=0.05){
  ifelse(a==0, runif(length(a),0,jitt), runif(length(a),1-jitt,1))
}
plot(x,y)
curve(invlogit(-24+0.4*x),add=TRUE)

```

2. Suppose the midterm scores were transformed to have a mean of 0 and standard deviation of 1. What would be the equation of the logistic regression using these transformed scores as a predictor?

$z=\frac{x-60}{15}$

$x=15z+60$

$logit(\pi)=-24+0.4(15z+60)=6z$

3. Create a new predictor that is pure noise (for example, in R you can create `newpred <- rnorm (n,0,1)`). Add it to your model. How much does the deviance decrease?

```{r}
newpred<-rnorm(50,0,1)
f_new<-function(x){
  result=invlogit(-24+0.4*x+newpred)
  return(result)
}
pi_new<-f_new(x)
pi<-f(x)
deviance<-function(y,pi){
  a<-y[y==0]
  pi1<-pi[y==0]
  b<-y[y==1]
  pi2<-pi[y==1]
  d1<-2*sum((1-a)*(1-a)/(1-pi1))
  d2<-2*sum(b*log(b/pi2))
  return(d1+d2)
}
deviance(y,pi) - deviance(y,pi_new)#because y is calculated depending on f(x),so f is the most fitted model for (x,y)
```

### Logistic regression

You are interested in how well the combined earnings of the parents in a child's family predicts high school graduation. You are told that the probability a child graduates from high school is 27% for children whose parents earn no income and is 88% for children whose parents earn $60,000. Determine the logistic regression model that is consistent with this information. (For simplicity you may want to assume that income is measured in units of $10,000).

$\frac{e^{\alpha}}{1+e^{\alpha}}=0.27$

$\alpha=log(0.27/0.73)=-0.99$

$\frac{e^{\alpha+6\beta}}{1+e^{\alpha+6\beta}}=0.88$

$\beta=(log(0.88/0.12)-\alpha)/6=0.50$

$\pi=logit^{-1}(-0.99+0.5x)$

### Latent-data formulation of the logistic model: 
take the model $Pr(y = 1) = logit^{-1}(1 + 2x_1 + 3x_2)$ and consider a person for whom $x_1 = 1$ and $x_2 = 0.5$. Sketch the distribution of the latent data for this person. Figure out the probability that $y=1$ for the person and shade the corresponding area on your graph.
```{r}
epsilon<-rlogis(1000,0,1)
z_latent<-1+2+1.5+epsilon
density<-dlogis(z_latent)
data<-data.frame(cbind(epsilon,z_latent,density))

ggplot(data,mapping=(aes(x=z_latent,y=density)))+geom_line()+geom_area(mapping=aes(x=ifelse(z_latent>=0,z_latent,0)),fill="red")+ylim(0,0.5)

```
### Limitations of logistic regression: 

consider a dataset with $n = 20$ points, a single predictor x that takes on the values $1, \dots , 20$, and binary data $y$. Construct data values $y_{1}, \dots, y_{20}$ that are inconsistent with any logistic regression on $x$. Fit a logistic regression to these data, plot the data and fitted curve, and explain why you can say that the model does not fit the data.

```{r}
x<-seq(1,20,1)
y<-rep(1,20)
model<-glm(y~x,family = binomial(link = "logit"))
summary(model)
plot(x,y)
curve(invlogit(cbind(1,x) %*% coef(model)),add=TRUE)
binnedplot(fitted(model),resid(model,type="response"))

```
*Because the residuals are not falling in the 95% confidence range.*
### Identifiability: 

the folder nes has data from the National Election Studies that were used in Section 5.1 of the Gelman and Hill to model vote preferences given income. When we try to fit a similar model using ethnicity as a predictor, we run into a problem. Here are fits from 1960, 1964, 1968, and 1972:

```{r, echo=FALSE}
nes5200_dt_d<-nes5200_dt[ presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_d<-nes5200_dt_d[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_d$income <- droplevels(nes5200_dt_d$income)

nes5200_dt_d$income <- as.integer(nes5200_dt_d$income)
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1960)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1964)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1968)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1972)))

newdata<-subset(nes5200_dt_d,year %in% c(1960,1964,1968,1972) & !is.na(black))
newdata$year<-factor(newdata$year)
newdata$vote_rep<- factor(newdata$vote_rep,levels=c(0,1),labels=c("Democrat","Republican"))
newdata$black<- factor(newdata$black,levels=c(0,1),labels=c("Not Black","Black"))
ggplot(newdata)+ aes(x=black,y=vote_rep,color=vote_rep)+geom_jitter()+ facet_grid(.~year)+scale_color_manual(values=c("blue","red"))+ ylab("")+xlab("")

```


What happened with the coefficient of black in 1964? Take a look at the data and figure out where this extreme estimate came from. What can be done to fit the model in 1964?

*We can find that in 1964 there was no black vote for Republican.*

*To solve the problem,We can do a subset analysis without considering the black population.*


# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

